2017-01-11 21:01:21 [program started on Wed Jan 11 21:01:21 2017] 
2017-01-11 21:01:21 [command line arguments] 
2017-01-11 21:01:21 seed 123 
2017-01-11 21:01:21 earlyStop 5 
2017-01-11 21:01:21 initWeight 0.08 
2017-01-11 21:01:21 LRDecay 0 
2017-01-11 21:01:21 batchSize 50 
2017-01-11 21:01:21 numLayers 2 
2017-01-11 21:01:21 decayRate 2 
2017-01-11 21:01:21 model LSTM 
2017-01-11 21:01:21 constBatchSize false 
2017-01-11 21:01:21 gradClip 5 
2017-01-11 21:01:21 LR 0.002 
2017-01-11 21:01:21 seqLength 50 
2017-01-11 21:01:21 load  
2017-01-11 21:01:21 nGPU 1 
2017-01-11 21:01:21 epochDecay 5 
2017-01-11 21:01:21 devid 1 
2017-01-11 21:01:21 save /home/dannysem@st.technion.ac.il/DeepLearning/ex3/dataset/recurrent.torch/examples/language/Results/WedJan1121:01:202017 
2017-01-11 21:01:21 checkpoint 0 
2017-01-11 21:01:21 type cuda 
2017-01-11 21:01:21 momentum 0 
2017-01-11 21:01:21 rnnSize 128 
2017-01-11 21:01:21 weightDecay 0 
2017-01-11 21:01:21 threads 8 
2017-01-11 21:01:21 optimization rmsprop 
2017-01-11 21:01:21 dropout 0.5 
2017-01-11 21:01:21 shuffle false 
2017-01-11 21:01:21 optState false 
2017-01-11 21:01:21 epoch 100 
2017-01-11 21:01:21 [----------------------] 
2017-01-11 21:01:21 
==> Network 
2017-01-11 21:01:21 nn.Sequential {
  [input -> (1) -> (2) -> (3) -> output]
  (1): nn.LookupTable
  (2): nn.Sequential {
    [input -> (1) -> (2) -> (3) -> (4) -> output]
    (1): nn.LSTM(128 -> 128, 256)
    (2): nn.Dropout(0.500000)
    (3): nn.LSTM(128 -> 128, 256)
    (4): nn.Dropout(0.500000)
  }
  (3): nn.TemporalModule {
    [input -> (1) -> output]
    (1): nn.Linear(128 -> 10000)
  }
} 
2017-01-11 21:01:21 
==>1553168 Parameters 
2017-01-11 21:01:21 
==> Criterion 
2017-01-11 21:01:21 nn.CrossEntropyCriterion 
2017-01-11 21:01:21 
Epoch 1
 
2017-01-11 21:01:58 
Training Perplexity: 482.1266784668 
2017-01-11 21:01:59 
Validation Perplexity: 312.38323974609 
2017-01-11 21:02:01 
Test Perplexity: 296.47521972656 
2017-01-11 21:02:01 
Epoch 2
 
2017-01-11 21:02:36 
Training Perplexity: 306.63668823242 
2017-01-11 21:02:37 
Validation Perplexity: 247.6175994873 
2017-01-11 21:02:39 
Test Perplexity: 233.58322143555 
2017-01-11 21:02:39 
Epoch 3
 
2017-01-11 21:03:14 
Training Perplexity: 254.15960693359 
2017-01-11 21:03:15 
Validation Perplexity: 215.15934753418 
2017-01-11 21:03:17 
Test Perplexity: 201.73161315918 
2017-01-11 21:03:17 
Epoch 4
 
2017-01-11 21:03:52 
Training Perplexity: 224.21250915527 
2017-01-11 21:03:53 
Validation Perplexity: 194.32206726074 
2017-01-11 21:03:55 
Test Perplexity: 182.18441772461 
2017-01-11 21:03:55 
Epoch 5
 
2017-01-11 21:04:30 
Training Perplexity: 204.78584289551 
2017-01-11 21:04:31 
Validation Perplexity: 178.86958312988 
2017-01-11 21:04:33 
Test Perplexity: 168.14471435547 
2017-01-11 21:04:33 Learning Rate decreased to: 0.001 
2017-01-11 21:04:33 
Epoch 6
 
2017-01-11 21:05:08 
Training Perplexity: 188.93942260742 
2017-01-11 21:05:09 
Validation Perplexity: 170.10427856445 
2017-01-11 21:05:11 
Test Perplexity: 159.66276550293 
2017-01-11 21:05:11 Learning Rate decreased to: 0.0005 
2017-01-11 21:05:11 
Epoch 7
 
2017-01-11 21:05:46 
Training Perplexity: 181.12921142578 
2017-01-11 21:05:47 
Validation Perplexity: 166.18925476074 
2017-01-11 21:05:49 
Test Perplexity: 156.02955627441 
2017-01-11 21:05:49 Learning Rate decreased to: 0.00025 
2017-01-11 21:05:49 
Epoch 8
 
2017-01-11 21:06:24 
Training Perplexity: 177.31190490723 
2017-01-11 21:06:25 
Validation Perplexity: 164.14575195312 
2017-01-11 21:06:26 
Test Perplexity: 154.30451965332 
2017-01-11 21:06:27 Learning Rate decreased to: 0.000125 
2017-01-11 21:06:27 
Epoch 9
 
2017-01-11 21:07:02 
Training Perplexity: 175.01040649414 
2017-01-11 21:07:03 
Validation Perplexity: 163.33374023438 
2017-01-11 21:07:04 
Test Perplexity: 153.59106445312 
2017-01-11 21:07:05 Learning Rate decreased to: 6.25e-05 
2017-01-11 21:07:05 
Epoch 10
 
2017-01-11 21:07:40 
Training Perplexity: 173.90542602539 
2017-01-11 21:07:41 
Validation Perplexity: 162.88900756836 
2017-01-11 21:07:42 
Test Perplexity: 153.20104980469 
2017-01-11 21:07:43 Learning Rate decreased to: 3.125e-05 
2017-01-11 21:07:43 
Epoch 11
 
2017-01-11 21:08:18 
Training Perplexity: 173.43167114258 
2017-01-11 21:08:19 
Validation Perplexity: 162.70263671875 
2017-01-11 21:08:20 
Test Perplexity: 153.04196166992 
2017-01-11 21:08:21 Learning Rate decreased to: 1.5625e-05 
2017-01-11 21:08:21 
Epoch 12
 
2017-01-11 21:08:56 
Training Perplexity: 173.15351867676 
2017-01-11 21:08:57 
Validation Perplexity: 162.61289978027 
2017-01-11 21:08:58 
Test Perplexity: 152.94660949707 
2017-01-11 21:08:59 Learning Rate decreased to: 7.8125e-06 
2017-01-11 21:08:59 
Epoch 13
 
2017-01-11 21:09:34 
Training Perplexity: 172.95324707031 
2017-01-11 21:09:35 
Validation Perplexity: 162.60429382324 
2017-01-11 21:09:36 
Test Perplexity: 152.92436218262 
2017-01-11 21:09:37 Learning Rate decreased to: 3.90625e-06 
2017-01-11 21:09:37 
Epoch 14
 
2017-01-11 21:10:12 
Training Perplexity: 173.22859191895 
2017-01-11 21:10:13 
Validation Perplexity: 162.57467651367 
2017-01-11 21:10:14 
Test Perplexity: 152.90483093262 
2017-01-11 21:10:15 Learning Rate decreased to: 1.953125e-06 
2017-01-11 21:10:15 
Epoch 15
 
2017-01-11 21:10:50 
Training Perplexity: 172.93469238281 
2017-01-11 21:10:51 
Validation Perplexity: 162.55854797363 
2017-01-11 21:10:52 
Test Perplexity: 152.8927154541 
2017-01-11 21:10:53 Learning Rate decreased to: 9.765625e-07 
2017-01-11 21:10:53 
Epoch 16
 
2017-01-11 21:11:28 
Training Perplexity: 172.92645263672 
2017-01-11 21:11:29 
Validation Perplexity: 162.55567932129 
2017-01-11 21:11:30 
Test Perplexity: 152.88812255859 
2017-01-11 21:11:30 Learning Rate decreased to: 4.8828125e-07 
2017-01-11 21:11:30 
Epoch 17
 
2017-01-11 21:12:06 
Training Perplexity: 173.05851745605 
2017-01-11 21:12:07 
Validation Perplexity: 162.55351257324 
2017-01-11 21:12:08 
Test Perplexity: 152.8861541748 
2017-01-11 21:12:08 Learning Rate decreased to: 2.44140625e-07 
2017-01-11 21:12:08 
Epoch 18
 
2017-01-11 21:12:44 
Training Perplexity: 172.81384277344 
2017-01-11 21:12:45 
Validation Perplexity: 162.55187988281 
2017-01-11 21:12:46 
Test Perplexity: 152.88478088379 
2017-01-11 21:12:46 Learning Rate decreased to: 1.220703125e-07 
2017-01-11 21:12:46 
Epoch 19
 
2017-01-11 21:13:22 
Training Perplexity: 172.61849975586 
2017-01-11 21:13:23 
Validation Perplexity: 162.55102539062 
2017-01-11 21:13:24 
Test Perplexity: 152.88397216797 
2017-01-11 21:13:24 Learning Rate decreased to: 6.103515625e-08 
2017-01-11 21:13:24 
Epoch 20
 
2017-01-11 21:14:00 
Training Perplexity: 173.09928894043 
2017-01-11 21:14:01 
Validation Perplexity: 162.55049133301 
2017-01-11 21:14:02 
Test Perplexity: 152.88360595703 
2017-01-11 21:14:02 Learning Rate decreased to: 3.0517578125e-08 
2017-01-11 21:14:02 
Epoch 21
 
2017-01-11 21:14:38 
Training Perplexity: 172.87649536133 
2017-01-11 21:14:39 
Validation Perplexity: 162.55032348633 
2017-01-11 21:14:40 
Test Perplexity: 152.88354492188 
2017-01-11 21:14:40 Learning Rate decreased to: 1.52587890625e-08 
2017-01-11 21:14:40 
Epoch 22
 
2017-01-11 21:15:16 
Training Perplexity: 172.93478393555 
2017-01-11 21:15:17 
Validation Perplexity: 162.55017089844 
2017-01-11 21:15:18 
Test Perplexity: 152.88346862793 
2017-01-11 21:15:18 Learning Rate decreased to: 7.62939453125e-09 
2017-01-11 21:15:18 
Epoch 23
 
2017-01-11 21:15:54 
Training Perplexity: 172.91070556641 
2017-01-11 21:15:55 
Validation Perplexity: 162.55017089844 
2017-01-11 21:15:56 
Test Perplexity: 152.88339233398 
2017-01-11 21:15:56 Learning Rate decreased to: 3.814697265625e-09 
2017-01-11 21:15:56 
Epoch 24
 
2017-01-11 21:16:31 
Training Perplexity: 172.98434448242 
2017-01-11 21:16:33 
Validation Perplexity: 162.55009460449 
2017-01-11 21:16:34 
Test Perplexity: 152.88339233398 
2017-01-11 21:16:34 Learning Rate decreased to: 1.9073486328125e-09 
2017-01-11 21:16:34 
Epoch 25
 
2017-01-11 21:17:09 
Training Perplexity: 172.90327453613 
2017-01-11 21:17:10 
Validation Perplexity: 162.55009460449 
2017-01-11 21:17:12 
Test Perplexity: 152.88339233398 
2017-01-11 21:17:12 Learning Rate decreased to: 9.5367431640625e-10 
2017-01-11 21:17:12 
Epoch 26
 
2017-01-11 21:17:47 
Training Perplexity: 172.87252807617 
2017-01-11 21:17:48 
Validation Perplexity: 162.55009460449 
2017-01-11 21:17:50 
Test Perplexity: 152.88339233398 
2017-01-11 21:17:50 Learning Rate decreased to: 4.7683715820313e-10 
2017-01-11 21:17:50 
Epoch 27
 
2017-01-11 21:18:25 
Training Perplexity: 172.70263671875 
2017-01-11 21:18:26 
Validation Perplexity: 162.55009460449 
2017-01-11 21:18:28 
Test Perplexity: 152.88339233398 
2017-01-11 21:18:28 Learning Rate decreased to: 2.3841857910156e-10 
2017-01-11 21:18:28 
Epoch 28
 
2017-01-11 21:19:03 
Training Perplexity: 172.78105163574 
2017-01-11 21:19:04 
Validation Perplexity: 162.55009460449 
2017-01-11 21:19:06 
Test Perplexity: 152.88339233398 
2017-01-11 21:19:06 Learning Rate decreased to: 1.1920928955078e-10 
2017-01-11 21:19:06 
Epoch 29
 
2017-01-11 21:19:41 
Training Perplexity: 172.9479675293 
2017-01-11 21:19:42 
Validation Perplexity: 162.55009460449 
2017-01-11 21:19:44 
Test Perplexity: 152.88339233398 
2017-01-11 21:19:44 Learning Rate decreased to: 5.9604644775391e-11 
2017-01-11 21:19:44 
Epoch 30
 
2017-01-11 21:20:19 
Training Perplexity: 172.95523071289 
2017-01-11 21:20:20 
Validation Perplexity: 162.55009460449 
2017-01-11 21:20:22 
Test Perplexity: 152.88339233398 
2017-01-11 21:20:22 Learning Rate decreased to: 2.9802322387695e-11 
2017-01-11 21:20:22 
Epoch 31
 
2017-01-11 21:20:57 
Training Perplexity: 172.71722412109 
2017-01-11 21:20:58 
Validation Perplexity: 162.55009460449 
2017-01-11 21:20:59 
Test Perplexity: 152.88339233398 
2017-01-11 21:21:00 Learning Rate decreased to: 1.4901161193848e-11 
2017-01-11 21:21:00 
Epoch 32
 
2017-01-11 21:21:35 
Training Perplexity: 172.86833190918 
2017-01-11 21:21:36 
Validation Perplexity: 162.55009460449 
2017-01-11 21:21:37 
Test Perplexity: 152.88339233398 
2017-01-11 21:21:38 Learning Rate decreased to: 7.4505805969238e-12 
2017-01-11 21:21:38 Best Iteration was 27, With a validation loss of: 5.0909862025031 
2017-01-11 21:21:38 
2017-01-11 21:21:38 [1;30m[1222.1040s][0m 
